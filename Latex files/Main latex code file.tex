%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[journal,article,submit,pdftex,moreauthors]{Definitions/mdpi} 

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, analytica, analytics, anatomia, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coasts, coatings, colloids, colorants, commodities, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, entomology, entropy, environments, environsciproc, epidemiologia, epigenomes, est, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, foundations, fractalfract, fuels, future, futureinternet, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, grasses, gucdd, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijpb, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidneydial, kinasesphosphatases, knowledge, land, languages, laws, life, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, microplastics, minerals, mining, modelling, molbank, molecules, mps, msf, mti, muscles, nanoenergyadv, nanomanufacturing,\gdef\@continuouspages{yes}} nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, %%nri, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, %oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacoepidemiology,\gdef\@ISSN{2813-0618}\gdef\@continuous pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, publications, quantumrep, quaternary, qubs, radiation, reactions, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, thermo, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wind, women, world, youth, zoonoticdis 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2024}
\copyrightyear{2024}
%\externaleditor{Academic Editor: Firstname Lastname}
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\usepackage{subcaption}
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For corrected papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates


%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Integrated modeling and target classification based on mmWave SAR and CNN approach}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Integrated modeling and target classification based on mmWave SAR and CNN approach}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0000-0000-000X} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Chandra Wadde $^{1,3}$, Gayatri Routhu $^{1,3}$, Mark Clemente-Arenas$^{2}$, Gummadi Surya Prakash$^{1,3}$ and Rupesh Kumar $^{3,}$* }

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Chandra Wadde, Gayatri Routhu, Mark Clemente-Arenas, Gummadi Surya Prakash and Rupesh Kumar}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Wadde, C.; Routhu, G.; Clemente-Arenas, M.; Gummadi, S.P.; Kumar, R.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Ph.D Scholar, Department of Electronics and Communication Engineering(ECE), SRM University AP; chandra\_vadde@srmap.edu.in (C.W), gayatri\_routhu@srmap.edu.in (G.R),  suryaprakash\_gummadi@srmap.edu.in (G.S.P), \\
$^{2}$ \quad Electronics Circuits and Systems Research Group, Universidad Nacional Tecnologica de Lima Sur UNTELS, Villa El Salvador, Lima, Peru, mclemente@untels.edu.pe (M.C), \\
$^{3}$ \quad WSI Lab \& 6G Research Lab, SRM University AP }
% Contact information of the corresponding author
\corres{Correspondence: rupesh.k@srmap.edu.in (R.K)}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  % Current address should not be the same as any items in the Affiliation section.
%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{This research introduces a numerical modeling approach utilizing millimeter-wave (mm-Wave), Frequency-Modulated Continuous-Wave (FMCW) radar to reconstruct and classify five weapon types: grenades, knives, guns, iron rods, and wrenches. By collecting 1000 images of these weapons from diverse online sources, the proposed method uses these 1000 online images to generate 3605 samples in the MATLAB environment to create reflectivity-added images. This work considered background reflectivity as 0 to 0.3 (0 being a perfect absorber) and object reflectivity as 0.8 to 1 (perfect material). These images are employed to reconstruct high-resolution weapon profiles using a monostatic 2-dimensional (2D) Synthetic Aperture Radar (SAR) imaging technique. Subsequently, the reconstructed images were classified with the help of the Convolutional Neural Network (CNN) algorithm in a Python environment. The CNN architecture consists of 10 layers, including multiple convolutional layers, pooling layers, and fully connected layers, designed to effectively extract features and perform classification. The CNN model achieved high accuracy, with precision and recall values exceeding 98\% across most categories, demonstrating the robustness and reliability of the model. This approach shows considerable promise for enhancing security screening technologies across a range of applications.}

% Keywords
\keyword{mmWave FMCW radar, Synthetic Aperture Radar(SAR), reflectivity-added images, Convolutional Neural Networks (CNN).} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine and Smart Cities
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%

%\noindent This is an obligatory section in “Advances in Respiratory Medicine'' and ``Smart Cities”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\setcounter{section}{-1} %% Remove this when starting to work on the template.
%\section{How to Use this Template}

%The template details the sections that can be used in a manuscript. Note that the order and names of article sections may differ from the requirements of the journal (e.g., the positioning of the Materials and Methods section). Please check the instructions on the authors' page of the journal to verify the correct order and names. For any questions, please contact the editorial office of the journal or support@mdpi.com. For LaTeX-related questions please contact latex@mdpi.com.%\endnote{This is an endnote.} % To use endnotes, please un-comment \printendnotes below (before References). Only journal Laws uses \footnote.

% The order of the section titles is different for some journals. Please refer to the "Instructions for Authors” on the journal homepage.


\section{Introduction}
%\textcolor{red}{
%1) Literature Survey: Speak about the solution reported.
%2) Define your problem Statement}
\IEEEPARstart{T}{he} advancement of radar technology, particularly in the millimeter-wave (mm-wave) frequency bands spanning 30 GHz to 300 GHz, has significantly expanded the capabilities of high-resolution imaging across diverse applications, including automotive safety \cite{alvarez2019millimeter, smith2022radar}, healthcare monitoring \cite{li2020wireless, zhang2022neural, guo2021deep}, and, most notably, security and surveillance \cite{liu2021deep, chen2021image, park2020high}. The unique penetration capabilities of mmWave radar, which allow it to pass through materials like clothing, ceramics, and luggage without compromising privacy \cite{cianca2013radar, lee2020radar, sun2019machine}, make it a powerful tool for non-invasive inspection. Among the various types of radar systems, mmWave Frequency Modulated Continuous Wave (FMCW) radars, particularly those operating in the 77 GHz to 81 GHz range, have demonstrated exceptional efficacy in generating good-resolution images crucial for precise object recognition and classification \cite{gao2020synthetic, zheng2019robust, huang2022millimeter}. This ability to deliver detailed imagery, even in challenging environments \cite{liu2023dynamic, zhang2023radar, yu2023machine}, positions mmWave FMCW radar as a critical technology for advanced imaging applications, enhancing both the resolution and reliability of security screening and surveillance systems \cite{liu2022fusion, li2022deep, chen2023real}.


Previous works in the field of image reconstruction using mmWave FMCW radar and SAR techniques have primarily focused on enhancing the accuracy and robustness of target detection and classification by leveraging advanced radar signal processing and deep learning models. For instance,\cite{liu2021deep, chen2021image} utilized a broad frequency range from 76 GHz to 81 GHz and integrated Deep Residual Networks (ResNet) to classify targets based on Synthetic Aperture Radar images. Their approach benefited from ResNet's deep architecture, which is particularly adept at extracting complex features from high-dimensional radar data, thereby improving classification accuracy, especially in challenging conditions involving noise and clutter. However, the complexity of the network also meant increased computational demands, data size and processing times, which could limit real-time applicability. Similarly, \cite{gao2020synthetic} explored the use of SAR imaging combined with deep learning techniques for high-resolution radar image reconstruction. This work emphasized enhancing the resolution and clarity of SAR images to better identify objects, employing complex neural networks to manage the intricacies of radar signal patterns. While effective in improving image quality, the approach required extensive data preprocessing and high computational power, which can be a drawback for real-time or resource-constrained applications.

Other studies, such as those by \cite{he2016deep, sun2019machine}, also focused on improving feature extraction and classification accuracy through advanced neural network architectures like ResNet. These networks were trained on large datasets to generalize well across various scenarios, including different object types and environmental conditions. However, such methods often involved broader frequency bands and more layers in the network, contributing to greater computational complexity and making them less feasible for real-time deployment in security screening applications.

Data creation for machine learning models, particularly in the context of mmWave FMCW radar and SAR imaging, poses significant challenges, as highlighted in existing research. Previous studies, such as those by, have underscored the difficulties in generating and curating datasets that are both comprehensive and representative. These complexities stem from the need to collect high-quality radar images under various real-world conditions, including different environments, angles, and object orientations, which often necessitate extensive manual efforts in collection and labeling. Additionally, simulating realistic radar reflectivity for diverse materials and surfaces further complicates the data creation process, requiring advanced preprocessing and augmentation techniques to produce reliable training datasets. The scarcity of publicly available radar image datasets exacerbates these challenges, forcing reliance on synthetic data or limited real-world samples that may not fully capture the intricacies of actual environments.

This model addresses these challenges by introducing an innovative approach to data creation that integrates 1000 online resource images of five different weapons—grenades, knives, guns, iron rods, and wrenches, preprocessing with realistic reflectivity simulation, By narrowing the frequency range to optimize radar performance and employing SAR imaging technique, our study offers a more streamlined and efficient solution for generating high-quality training data by utilizing MATLAB to generate a large dataset of 3605 reconstructed images are generated and stored for further processing. The CNN algorithm is chosen for its exceptional ability to automatically learn and extract complex features from high-resolution images, making it ideal for accurately classifying reconstructed mmWave FMCW radar images of various reconstructed weapon images.

The main important contributions of this research work are summarized as follows:
\begin{enumerate}
    \item \textbf{Reflectivity Mapping and Standardization of Online Data Set:} The study involves collecting 1000 online images of weapons from various online resources and generating 3605 samples with reflectivity mapping and standardization in prepossess phase. These 3605 reflective-added images are used for further image reconstruction using SAR imaging techniques.
    \item \textbf{Utilization of mmWave FMCW Radar with 2D SAR Imaging:} The work reconstructs preprocessed online resource images using mmWave FMCW radar combined with 2D SAR imaging techniques.
    \item \textbf{ML Classification Approach:} The reconstructed images are classified using machine learning techniques, specifically CNNs, which enhance the classification accuracy.
    \item \textbf{Complete Analytical Approach:} The approach employs SAR imaging for high-resolution reconstruction of preprocessed weapon images and CNNs for accurate classification. It achieves over 98\% accuracy, offering a non-invasive, high-resolution alternative to traditional security screening methods with enhanced capabilities to detect weapons through non-metallic surfaces.
\end{enumerate}

The hardware configuration included an NVIDIA GeForce RTX 3060 for accelerating the training process of the Convolutional Neural Network. A high-performance CPU with 32GB RAM and storage capacity of 1.5TB, running Windows 11 Pro is utilized to handle the demanding data preprocessing tasks. Despite these powerful tools, integrating the image reconstruction and classification workflows seamlessly proved to be a complex and demanding process.

The rest of the paper is organized as follows: Section II provides an overview of the framework, detailing the foundational elements and methodologies used in this study. Section III delves into data collection and preprocessing techniques, focusing on the generation of reflective-added images. In Section IV, the modeling of the mmWave FMCW radar is described in detail, including the specific parameters and configurations employed. Section V presents the machine learning technique used for classifying the reconstructed images, particularly the use of a CNN algorithm. Section VI discusses the results and offers a comprehensive analysis of the results. In Section VII, a comparison with previous works is made, highlighting the advancements and contributions of this study. Finally, Section VIII concludes the paper, summarizing the key outcomes and suggesting directions for future research.



\section{Framework Overview}
The research work is outlined in three phases as shown in Figure 1.  Phase 1 is detailed about online image gathering and preprocessing, Phase 2 explains the SAR imaging technique to reconstruct images, and Phase 3 gives an explains CNN classification of reconstructed images.  This process is meticulously designed to enhance the accuracy and reliability of security screening technologies, addressing the limitations of traditional methods. The workflow of this research is shown in Table 1.

\begin{figure}[h]
  \centering
 \includegraphics[width=0.5\textwidth]{fig11.jpg}
  \caption{Modeling mmWave Imaging}
  \label{Figure:}
\end{figure}


\begin{table}[h]
\centering
\caption{Work flow}
\label{tab:mmWave FMCW_Spec}
\begin{tabular}{l}
\hline
\textbf{Step by step overview workflow}  \\ \hline
    1. Online Images Collection and Preprocessing  \\
    2. mmWave FMCW radar modeling and Image Reconstruction\\
    3. Generate Reflectivity added Images\\
    4. Reconstruct Images using SAR imaging Technique \\
    5. Save the reconstructed images in folder\\
    6. Import reconstructed images for classification. \\ 
    7. Normalize, reshape, and design CNN Architecture\\
    8. Performance Evaluation and Analysis\\
    \hline\\
    \end{tabular}
\end{table}


\subsection*{Phase 1: Image Preprocessing}

The first phase begins with the acquisition of images of different weapons from different online resources, which represents the initial input for the system. This work collected 1000 different images which are different in dimensions. The primary goal of this phase is data collection, reflectivity mapping, and standardization of online resource images in uniform size. Data collection and preprocessing is a crucial step in preparing raw data for subsequent analysis. Gathering different images which are different in dimensions then these images are further preprocessing such as resizing, normalization, and possibly even contrast enhancement to ensure that the image is in an optimal state for further processing. This step is vital as it enhances the image quality, ensuring that the subsequent processes can be carried out effectively. The output of this phase is a reflective-added image, which incorporates reflectivity data to simulate how the object would interact with radar signals.

\subsection*{Phase 2: mmWave FMCW Image Reconstruction using SAR}

In the second phase, the work focuses on the preprocessed images to image reconstruction using mmWave FMCW radar and SAR imaging technique. The reflective-added image generated in the first phase is fed into this stage. Here, the SAR imaging technique is applied to reconstruct a high-resolution image that mimics the radar's interaction with the object. SAR is a form of radar used to create detailed images or maps of objects or landscapes. It operates by sending out radar pulses and capturing the echoes that return after bouncing off the object. The phase shifts in the returning signals are analyzed to produce a high-resolution image. The reconstructed image produced by this phase is critical as it represents a synthetic view of the object as if captured by the mmWave FMCW radar, which is essential for accurate object detection and classification in the subsequent phase.

\subsection*{Phase 3: Classification using Machine Learning}

The third phase is centered on the classification of the reconstructed images using machine learning algorithms in a Python environment. The reconstructed image obtained from Phase 2 serves as the input to a machine-learning model. CNN algorithm is chosen for this study due to good performance in image feature extraction and classification. Because CNNs can learn and extract features from input images using multiple layers of convolution and pooling, they are very effective in image classification tasks. In this phase, the CNN analyzes the reconstructed image, identifying patterns and features that correspond to specific classes of objects. For instance, in security applications, the CNN would be trained to distinguish between different types of weapons or other objects of interest. The final output of this phase is the classification performance, which indicates the effectiveness of the CNN in accurately identifying and categorizing the objects based on the reconstructed radar images.

\section{Data collection and Preprocessing to Generate Reflectivity-Added Image}

This work gathered online resource images of different weapons from different online resources. The choice to use online resource images as the base for generating reflective-added images was driven by the need for a comprehensive and varied dataset that reflects real-world conditions. These gathered online resource images are preprocessed and utilized for reconstruction in further process. 


\begin{figure}[h]
    \centering
    % First image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Fig.2.jpg}
        \Centering
        \caption*{(a)}
    \end{minipage}
    \hfill
    % Second image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig3.jpg}
        \Centering
        \caption*{(b)}
    \end{minipage}
    \caption{(a) Proposed workflow for reflectivity mapping and size standardization and (b) Reflectivity Mapping and size standardization: Preprocessing.}
    \label{fig:side_by_side}
\end{figure}

Figure 2a illustrates a detailed workflow for generating reflective-added images from online resource images, it is an important step in preparing data for mmWave FMCW radar imaging and classification tasks. The process begins with the loading of the online resource image, which serves as the raw input for the entire procedure. This image, consisting of an object such as a weapon, is then subjected to a series of preprocessing steps. Preprocessing is essential for ensuring that the data is in a suitable format for the application of reflectivity. These images are preprocessed to generate reflective added images followed by resizing, and normalizing pixel values are employed to create a uniform and clean image, reducing any potential artifacts that could interfere with later stages of the process. 

%normalizing pixel values, and removing noise are employed to create a uniform and clean image, reducing any potential artifacts that could interfere with later stages of the process.


 In the preprocessing step, reflectivity is added, which simulates how radar signals would interact with the object depicted in the image. Reflectivity is a measure of the mmWave FMCW radar signal reflected back from the object, and this step converts a regular photographic image into a format usable for mmWave FMCW radar-based analysis. Specifically, the reflectivity \(\rho\) is expressed mathematically as:
 \begin{equation}
\rho(x, y) = \frac{I(x, y)}{I_{\text{max}}}
 \end{equation}
where \(I(x, y)\) represents the pixel intensity at position \((x, y)\), and \(I_{\text{max}}\) is the maximum pixel intensity in the image. This normalization step ensures that reflectivity values fall within a range suitable for simulation. Following this, contrast enhancement is applied to improve the visibility of features in the image, ensuring that differences in reflectivity across the object are more pronounced. This step is particularly important for objects with subtle reflectivity variations, making it easier to identify and analyze key features.


%\begin{figure}[h]
%  \centering
% \includegraphics[width=0.5\textwidth]{fig3.jpg}
%  \caption{Reflectivity Mapping and size standardization: Preprocessing}
%  \label{Figure:}
%\end{figure}
The workflow then proceeds to initialize a grid and resize the image. Initializing a grid involves overlaying a grid structure onto the image, which helps in accurately mapping the spatial properties of the object within the image. This step, along with resizing the image of different dimensions of 'm x n' to standard dimensions of grid size '100 x 100' is shown in Figure 2b. To scale the object image to match the grid size, the following interpolation formula is generally used:
 \begin{equation}
     RI=interp(Original Image_{{m \times n}},New Size_{100 \times 100})
 \end{equation}

\noindent where \(RI\) represents Resized Image, \(New Size\) is modeled \(grid\_size\).%Ensures that the image data is compatible with the subsequent stages of the workflow. 

The resulting images, now embedded with reflectivity data, are saved in a labeled folder as JPEG format and plotted to provide a visual representation of how the object will appear under reflectivity property conditions. These plotted images serve as the input for the subsequent phases of mmWave FMCW radar image reconstruction and classification using the CNNs algorithm.

Five distinct weapon images from online resources images and reflective-added images after preprocessing are shown in Figures 3 through 5. Out of a total of 1000 images, some random images are chosen to analyze and understand the process. The parameters of reflective-added images considered in this study are shown in Table 2.\\


\begin{table}[H] 
\caption{Parameters of reflective-added images\label{tab5}}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{CCCC}
\toprule
%\textbf{}
   S.No & Parameter & Value & Property \\ \hline
    1 & Object Reflectivity & 0.8 to 1 & Perfect material  \\
        2 & Background Reflectivity & 0 to 0.3 & Perfect absorber\\
        3 & Image Size & m x n & Online resource image \\
        4 & grid\_size & 100 x 100 & Reflective-added image\\
     \bottomrule
\end{tabularx}
\end{table}


%\begin{table}
%    \caption{Reflective values}
%    \label{tab:my_label}
%    \begin{tabular}{c c c c}
%    \hline
%        1 & Object Reflectivity & 0.8 to 1 & Perfect material  \\
%        2 & Background Reflectivity & 0 to 0.3 & Perfect absorber\\ 
%        \hline
%   \end{tabular}   
%\end{table}


The first set of Figure 3a images represents grenades. The top row presents different types of grenades sourced from online resources, each labeled with their respective dimensions. The first grenade measures 70.5 mm in height and 50 mm in width, featuring a typical ribbed surface. The second grenade, with dimensions of 66.5 mm by 52 mm, has a smooth cylindrical body with a prominent safety pin and lever. The third grenade, measuring 44.5 mm by 79 mm, is compact and oval-shaped, often used for smoke or flash purposes. The fourth grenade, at 59.5 mm by 59.5 mm, has a streamlined design for specific tactical applications.


\begin{figure}[h]
    \centering
    % First image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{grenade1.jpg}
        \centering
        \caption*{(a)}
    \end{minipage}
    \hfill
    % Second image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gun.jpg}
        \caption*{(b)}
    \end{minipage}
    \caption{Online resource and preprocessed Reflective-added images based on proposed method (a) Grenades    (b) Guns}
    \label{fig:side_by_side}
\end{figure}







%\begin{figure}[h]
 % \centering
 % \includegraphics[width=0.5\textwidth]{grenade1.jpg}
 % \caption{Online resource and preprocessed Reflective-added grenade images based on proposed method}
 % \label{Figure:}
%\end{figure}
%The second row in Fig. 3 displays mmWave FMCW radar reflective-added images of these grenades, with background reflectivity and object reflectivity. These reflectivity-added images illustrate how radar signals interact with the grenade surfaces. The first reflectivity image shows high reflectivity on the ribbed surface, with the grenade appearing mostly red, indicating strong reflections. The second reflectivity image reveals a more complex pattern due to the cylindrical shape and metallic surface of the grenade, with variations in color reflecting different angles and surfaces interacting with the radar signals. The third reflectivity image, representing the compact oval-shaped grenade, shows a relatively uniform reflectivity, with distinct reflections from the safety pin and lever. The fourth reflectivity image of the streamlined grenade exhibits high reflectivity on its body, with some areas reflecting less due to their orientation or material properties.
The comparison of the online resource images of grenades with the corresponding reflective-added images after preprocessing is shown in Figure 3a. This preprocessing step ensures that the object has high reflectivity while the background has low reflectivity. The added reflectivity highlights the metallic and structural features of the grenade. The first reflectivity image shows high reflectivity on the ribbed surface, with the grenade appearing mostly red, indicating strong reflections. The second reflectivity image reveals a more complex pattern due to the cylindrical shape and metallic surface of the grenade, with variations in color reflecting different angles and surfaces interacting with the radar signals. The third reflectivity image, representing the compact oval-shaped grenade, shows a relatively uniform reflectivity, with distinct reflections from the safety pin and lever. The fourth reflectivity image of the streamlined grenade exhibits high reflectivity on its body, with some areas reflecting less due to their orientation or material properties.


%\begin{figure}[h]
 % \centering
%  \includegraphics[width=0.5\textwidth]{gun.jpg}
%  \caption{Online resource and preprocessed Reflective-added gun images based on proposed method}
%  \label{Figure:}
%\end{figure}

Likewise, the second weapon, the gun, focuses on online resource images and reflective-added images comparison. The analysis of gun images, showcasing their original appearances and corresponding preprocessed reflectivity mapping properties. The top row displays images of various guns sourced from online resources, with dimensions labeled. The first gun is a revolver measuring 211 mm in length and 141 mm in height, featuring a wooden grip and metallic barrel. The second is a compact semi-automatic pistol, 59.5 mm in length, known for its lightweight and ease of concealment. The third gun is a larger semi-automatic pistol, 69 mm in length and 51 mm in height, commonly used by military and police forces. The fourth is a subcompact pistol, 70 mm in length and 49.5 mm in height, ideal for concealed carry.

The second row in Figure 3b shows the reflective-added images of guns. Similar to the grenade images, the reflectivity values were added during preprocessing to emphasize the gun's structural components. The high reflectivity of the object allows for the detailed contours of the gun to be captured effectively, while the background remains less reflective. The revolver's metallic barrel exhibits high reflectivity, shown in red, and wooden grip in yellow tale, indicating strong reflections. The compact pistol shows a complex reflectivity pattern due to its polymer frame and metallic components. The larger semi-automatic pistol demonstrates relatively uniform reflectivity, with high reflectivity areas highlighting its metallic parts. The subcompact pistol displays high reflectivity on its body, with some variations due to orientation or material properties.



%The second row of Fig.4 shows how mmWave FMCW radar signals interact with the gun surfaces. The revolver's metallic barrel and wooden grip exhibit high reflectivity, shown in red and yellow colors, indicating strong reflections. The compact pistol shows a complex reflectivity pattern due to its polymer frame and metallic components. The larger semi-automatic pistol demonstrates relatively uniform reflectivity, with high reflectivity areas highlighting its metallic parts. The subcompact pistol displays high reflectivity on its body, with some variations due to orientation or material properties.



\begin{figure}[h]
    \centering
    % First image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{wrench1.jpg}
        \caption*{(a)}
    \end{minipage}
    \hfill
    % Second image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{ironrod1.jpg}
        \caption*{(b)}
    \end{minipage}
    \caption{Online resource and preprocessed Reflective-added images based on proposed method(a) Wrenches    (b) Iron-roads }
    \label{fig:side_by_side}
\end{figure}






%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.5\textwidth]{wrench1.jpg}
%  \caption{Online resource and preprocessed Reflective-added wrench images based on proposed method}
%  \label{Figure:}
%\end{figure}

The third set of images illustrates wrenches. Figure 4a presents a detailed analysis of wrench images, showing their original appearances and corresponding reflectivity properties. The first row represents images of various wrenches sourced from online resources, each labeled with their respective dimensions. The first wrench, measuring 59.5 mm in length, is a standard adjustable wrench with a red handle. The second wrench, also 59.5 mm, features an adjustable jaw with a black handle and red body. The third wrench, characterized by a wooden handle and metallic jaw, offers a different ergonomic grip. The fourth wrench, fully metallic and 59.5 mm in length, is designed for heavy-duty applications. The fifth wrench, slightly smaller at 52 mm, has a metal body with a plastic-coated handle for lighter tasks.

The reflective-added wrench images give detailed reflective values that are added to highlight the object's features, such as the metallic parts of the wrench. The uniformity in the object's reflectivity ensures that the wrench's shape and structure are well-defined. The background, with lower reflectivity, fades into insignificance, making the wrench stand out in the processed images. Similar to grenades reflective-added properties were observed in previous cases. The first wrench displays high reflectivity along its handles and jaws, indicated by red. Yellow tale color at the middle where the material has different shade effects. The second wrench has high reflectivity compared to the first wrench showing that it is the perfect material. The third wrench's wooden handle and metallic jaw create a unique reflectivity pattern, reflecting different materials' interactions. The fourth wrench shows high reflectivity throughout its fully metallic body, highlighting its uniform surface. The fifth wrench displays distinct reflectivity variations due to its plastic-coated handle, with metal parts showing higher reflectivity.




%Reflective-added images reveal how radar signals project the surface of different wrenches. The first and second wrenches display high reflectivity along their handles and jaws, indicated by red and yellow colors, suggesting consistent surface interaction with the radar signals. The third wrench's wooden handle and metallic jaw create a unique reflectivity pattern, reflecting different materials' interactions with the radar. The fourth wrench shows high reflectivity throughout its fully metallic body, highlighting its uniform surface. The fifth wrench displays distinct reflectivity variations due to its plastic-coated handle, with metal parts showing higher reflectivity.


%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.5\textwidth]{ironrod1.jpg}
%  \caption{Online resource and preprocessed Reflective-added iron rod images based on proposed method}
%  \label{Figure:}
%\end{figure}


The fourth set focuses on iron rods. Figure 4b presents a detailed analysis of iron rod images, comparing both their original and corresponding reflectivity properties. The images of various iron rods are sourced from online resources, each labeled with their respective dimensions. The first rod, 132 mm in length, is a plain cylindrical metal rod. The second rod, measuring 71.5 mm, features a smooth metallic surface. The third rod, at 70 mm, has a threaded surface for fastening purposes. The fourth rod, 63 mm in length, is another smooth cylindrical rod, while the fifth rod, 55.5 mm in length, has a textured surface for increased grip.


The second row in the fourth set of images shows reflective-added iron rods, where reflective values were added to enhance the object's visibility in the SAR imaging process. The preprocessing reflects the high reflectivity of the metal rod while keeping the background reflectivity low. This distinction between the object and background ensures that the iron rod's structural integrity and details are preserved and highlighted in the SAR-reconstructed images. The first and second rods display high, uniform reflectivity along their lengths, indicating consistent surface interactions, in the middle of these two images are considered as background reflectivity due to shade replication. The third rod, with its threaded surface, shows a complex reflectivity pattern with alternating high and low bands due to the thread ridges. The fourth rod exhibits a similar uniform reflectivity to the third rod, while the surface results in distinct reflectivity variations.

%The bottom row shows mmWave FMCW radar reflective-added images of these rods. The first and second rods display high, uniform reflectivity along their lengths, indicating consistent surface interactions. The third rod, with its threaded surface, shows a complex reflectivity pattern with alternating high and low bands due to the thread ridges. The fourth rod exhibits a similar uniform reflectivity to the second rod, while the fifth rod's textured surface results in distinct reflectivity variations.




\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{knife1.jpg}
  \caption{Online resource and preprocessed Reflective-added knife images based on proposed method}
  \label{Figure:}
\end{figure}

The final set of images represents knives. Knife images are thoroughly analyzed in Figure 5, which also displays the image's original appearances and preprocessed reflectivity characteristics. The first knife, measuring 211 mm in length and 141 mm in height, is a large combat knife with a serrated edge. The second knife, with the same dimensions, features a curved blade and a wooden handle, often used for agricultural tasks. The third knife is a cleaver with a wide, heavy blade, commonly used in culinary applications. The fourth knife is a machete with a long, curved blade for cutting through vegetation. The fifth knife, also 211 mm in length, has a hooked blade used for slicing motions in tasks like pruning.

The reflective-added knife images with high reflectivity assigned to the knife ensure that its sharp edges and metallic surfaces. The low-reflectivity background helps isolate the knife, making it the focal point of the image, which is essential for accurate object recognition and classification in subsequent analysis. The first knife's blade and handle show high reflectivity, indicated by red and yellow colors, suggesting consistent surface interaction. The second knife displays high reflectivity along its curved blade and handle, reflecting its distinctive shape. The third knife's cleaver blade creates a unique pattern due to its wide, heavy blade. The fourth knife's machete blade shows a consistent high reflectivity pattern, highlighting its uniform surface. The fifth knife’s hooked blade displays distinct reflectivity variations, with high reflectivity in metal parts and less reflection in curved areas.



%The bottom reflective-added images of these knives. The first knife's blade and handle show high reflectivity, indicated by red and yellow colors, suggesting consistent surface interaction. The second knife displays high reflectivity along its curved blade and handle, reflecting its distinctive shape. The third knife's cleaver blade creates a unique pattern due to its wide, heavy blade. The fourth knife's machete blade shows a consistent high reflectivity pattern, highlighting its uniform surface. The fifth knife’s hooked blade displays distinct reflectivity variations, with high reflectivity in metal parts and less reflection in curved areas.

The reflectivity-added images across all object categories provide a detailed understanding of the reflectivity of different materials when mmWave FMCW radar interacts with various surfaces and materials. High reflectivity areas indicated by red (0.8 to 1 as perfect material) show strong mmWave FMCW radar signal reflections, while lower reflectivity areas indicated by blue color (0 as perfect absorber) indicate weaker reflections. These variations are influenced by factors such as surface texture, material properties, and the angle of incidence. A total 3605 of reflective mapped and size standardization images are generated and saved for SAR reconstruction in the next phase. 

%This comprehensive analysis is important for enhancing mmWave FMCW radar imaging systems used in security applications, as it provides valuable insights into the detection and classification of diverse objects. Understanding these interactions allows for the development of more accurate and reliable radar-based detection methods, improving overall security and safety measures.

%\subsubsection{Visualization and Analysis}
%The online resource images and reflective-added images are visualized side by side for comparison.  

%The proposed numerical modeling approach successfully reconstructs radar images from mmWave FMCW radar data and prepares them for CNN-based classification. The reconstructed images exhibit realistic radar reflectivity properties, providing a robust dataset for training and validating the CNN. The effectiveness of the CNN in classifying these complex images demonstrates significant improvements over traditional methods, highlighting the potential for enhanced radar-based sensing and surveillance systems. 

\section{mmWave FMCW radar Modeling}
In order to reconstruct high-resolution images of different weapons, this research modeled a comprehensive modeling process using mmWave FMCW radar. The process included adding reflectivity properties to images and simulating SAR imaging. 
The provided block diagram Figure 6 illustrates the process of modeling mmWave FMCW radar for image reconstruction and classifying weapons using a CNN algorithm. In order to understand the diagram, which shows different transmitters (\(Tx\)) and receivers (\(RX\)) without taking into account any space between them, a monostatic SAR configuration is used. The reflective-added images are the targets for the mmWave FMCW radar to reconstruct as shown in Figure 6.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{modeling.jpg}
  \caption{Overall work Process}
  \label{Figure:}
\end{figure}
The process starts with the Synthesizer/Chirp Generator, which generates the frequency-modulated continuous wave (FMCW) signal [1],[23],[24], \cite{25,26,27,28}, \( S(t) \). A coupler/splitter then splits the generated signal into two equal parts: the transmitted signal \( S(t) \) and the reference signal \( S'(t) \).

The Transmitter (\(Tx\)), equipped with a high-gain directional antenna, sends the FMCW signal \( S(t) \) towards the target area. The signal reflects off the objects and is captured by the Receiver (\(Rx\)) as a backscattered signal \cite{chandrawadde2024modeling, chandrawadde2024ml}, which also uses a high-gain directional antenna to minimize noise and accurately capture the reflected signals. The backscattered signal carries important information about the target’s range and shape in terms of reflectivity. The reflected signal \( R(t) \) is mixed with the reference signal \( S'(t) \) in the Mixer, producing an intermediate frequency (IF) signal [23], [24],[26], \( Mix(t) \). 


To process the IF signal, an analog-to-digital converter (ADC) digitizes it, converting the analog signals into a digital format suitable for further processing by the digital signal processor (DSP) \cite{25,27,28}. The DSP undertakes real-time signal processing tasks, which include filtering, performing Fast Fourier Transform (FFT) operations \cite{29, 30, 31, 32}, and extracting range and shape information from the beat frequencies. 

The Reconstructed Images section displays the images reconstructed using the mmWave FMCW radar data, showing the detected shapes and details of the objects. These reconstructed images are then classified using the CNN Algorithm. The CNN algorithm processes these images to classify the objects, leveraging the spatial hierarchies of the features learned from the mmWave FMCW radar images. This integration of mmWave FMCW radar with advanced machine learning techniques demonstrates an effective method for weapon detection and classification.

\subsection{SAR Scanning Area}
This work utilized a monostatic SAR configuration \cite{chandrawadde2024modeling,chandrawadde2024ml}, where the transmitter and receiver are co-located, simplifying hardware design and data processing algorithms. The 2D SAR scanning area is reflected in Figure 7.


%\section{Numerical Modeling and Analysis: Methodology}

The 2D SAR imaging scanning area refers to the specific space within which the radar system operates to reconstruct object shapes. This area serves as the target scene for the radar, determining the range and coverage of the radar signals. In this study, MATLAB is used to model a 2D SAR scanning scenario as shown in Figure. 7. The grid size in the xy-plane is defined by Nd and Nr, with the transmit signal directed towards the target within the scanning area, and the step sizes in the xy-plane designated as the x-step and y-step. This approach allows us to comprehensively explore the SAR imaging process and its parameters.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth,height = 0.45\textwidth]{scanning.jpg}
  \caption{Modeled 2D SAR scanning area}
  \label{Figure:}
\end{figure}
\subsection{Numerical Modeling Approach}
The proposed numerical modeling and analysis method integrates mmWave radar imaging using the SAR imaging technique with CNN to classify the reconstructed images. The main focus is on detailing the mathematical formulations, analysis procedures, and computational techniques employed to validate the effectiveness of the model.

The mmWave FMCW radar parameters such as carrier frequency, chirp bandwidth, chirp time, and chirp slope are defined based on the radar system specifications. These parameters influence the mmWave radar signal's characteristics and its interaction with objects in the scanning area. A grid is established to simulate the mmWave radar’s field of view, with scanning points and step size in both xy-planes corresponding to different spatial locations within the grid as shown in Table 3.


\begin{table}[H]
\caption{mmWave Parameters Considered for Modeling.\label{tab2}}
\begin{adjustwidth}{-\extralength}{0cm}
    \centering
    \begin{tabularx}{\fulllength}{CCCC}
        \toprule
        \textbf{S.No} & \textbf{Parameter} & \textbf{Description} & \textbf{Value} \\
        \midrule
        1 & \textbf{c} & Speed of light & \(3 \times 10^8\) m/s \\
        2 & \textbf{max\_range} & Maximum range for the radar system & 100 mm \\
        3 & \textbf{object\_range} & Range of the object & 50 mm \\
        4 & \textbf{range\_resolution} & Range resolution of the radar system & 3.8 mm \\
        5 & \textbf{fc} & Carrier frequency & 77 GHz \\
        6 & \textbf{B} & Chirp bandwidth & \( \frac{c}{2 \times \text{range\_resolution}} \) \\
        7 & \textbf{Tchirp} & Chirp time & \( 5.5 \times \frac{2 \times \text{max\_range}}{c} \) \\
        8 & \textbf{slope} & Chirp slope & \( \frac{B}{Tchirp} \) \\
        9 & \textbf{Nd} & Number of scanning points in x-axis & 100 \\
        10 & \textbf{Nr} & Number of scanning points in y-axis & 100 \\
        11 & \textbf{grid\_size} & Total grid size & \( \text{Nd} \times \text{Nr} \) \\
        12 & \textbf{step\_sizex} & Step size along X-axis & 1.9 $(\lambda/2)$ \\
        13 & \textbf{step\_sizey} & Step size along Y-axis & 1.9 $(\lambda/2)$ \\
        14 & \textbf{num\_freq} & Number of frequencies to sum over & 10 \\
        \bottomrule
    \end{tabularx}
\end{adjustwidth}
\noindent{\footnotesize{}}
\end{table}




For each grid point, the reflectivity was calculated based on the property of the object from the mmWave FMCW radar and the object's location, providing a detailed reflectivity profile of the scene. The transmitted and received mmWave radar signals are modeled for each grid point, incorporating the effects of object reflectivity and range. These signals are processed using a two-dimensional Fast Fourier Transform (2D FFT) to reconstruct the image, effectively representing the object's reflectivity profile.

To reconstruct the spatial image of the target, the 2D FFT transforms the frequency-domain representation of the received radar signal into the spatial domain. Specifically, the range information is extracted along one dimension (frequency axis), while the Doppler shifts, which provide velocity information, are extracted along the orthogonal dimension. The inversion of this data into spatial coordinates leverages the relationship between the time delay \(d_T\) and the range of the object, expressed as \cite{liu2021deep, park2020high}:
\begin{equation}
    d_T = \frac{2R}{c}
\end{equation}

where \(R\) is the distance from the radar to the target and \(c\) is the speed of light. By performing the 2D FFT on the mixed signal, the spatial distribution of the object's reflectivity is obtained, effectively reconstructing the image of the target.

\subsection{Tx and Rx Signal Modeling}
The transmitted signal (\(Tx\)) and the received signal (\( Rx\)) are calculated based on the chirp parameters and the range values. The mathematical expression for the transmitted signal is derived as:

\begin{adjustwidth}{-\extralength}{0cm}
\begin{equation}
   \begin{aligned}
    Tx(x, y, z) = \sum_{x=1, y=1}^{x=100, y=100} \sum_{f_i=1}^{f_i =10} \exp \left( j \left( 2 \pi \left( f_i \cdot t_y + 
    \frac{S \cdot (t_y)^2}{2} \right) \cdot \frac{R(x, y, z)^2}{c} \right) \right)
    \end{aligned}
\end{equation}
\end{adjustwidth}


\noindent Where \(f_i\) represents the carrier frequency of the radar signal, which is incremented in steps of 0.1 GHz within a 1 GHz bandwidth, ranging from 77 GHz to 78 GHz. \( c \) is the speed of light. \(t_y\) represents the time variable corresponding to the \(y^{th}\) position. The chirp slope, which determines the rate of frequency change over time, is denoted by \(S\).  The chirp slope represents the frequency modulation over time squared is determined by \(S\cdot (t_y)^2\). \(R(x,y,z)\) represents the range to the target at coordinates \((x,y,z)\). It determines how far the target is from the radar system. \(z\) represents the fixed range of the object from radar. The backscattered signal is derived as:

\begin{adjustwidth}{-\extralength}{0cm}
\begin{equation}
   \begin{aligned}
        Rx(x, y, z) &= \sum_{x=1, y=1}^{x=100, y=100} \sum_{f_i=1}^{f_i=10} \alpha \left( \exp \left( j \cdot 2 \pi \left( f_i \cdot \left( t_y - T_d \right) + \frac{S \cdot (t_y)^2 - T_d}{2} \right) \cdot \frac{R(x, y, z)^2}{c} \right) \right)
   \end{aligned}
\end{equation}
\end{adjustwidth}

\noindent Where, the received signal \( Rx(x, y, z) \) represents the complex exponential form of the signal at position \( (x, y, z) \). The time delay is represented by \( T_d \). The chirp slope, which determines the rate of frequency change over time, is denoted by \(S\).  The chirp slope represents the frequency modulation over time squared is determined by \(S\cdot (t_y)^2\). The entire expression within the exponential function represents the phase of the received signal modulated by the range to the target.

The Mixer combines transmitted and received signals, that generate an intermediate frequency (IF) or beat frequency which is defined as follows. 

\begin{adjustwidth}{-\extralength}{0cm}
\begin{equation}
   {Mix}(x, y, z) = \sum_{x=1, y=1}^{x=m, y=m} \sum_{f_i=1}^{f_i=10} \alpha (x, y, z) \cdot {Tx}(x, y, z) \cdot {Rx}(x, y, z)
\end{equation}
\end{adjustwidth}



\noindent Where \(Mix(x,y)\) refers to the complex-valued signal at the grid position \((x,y)\). It represents the accumulated mixed signal at that specific point in the grid. \(\text{$\alpha$} (x,y)\) represents the presence of an object in the scanning area at position\((x,y)\). The mixed signal is then processed using a 2D FFT to transform it from the time domain to the frequency domain, followed by an inverse FFT to reconstruct the image in the spatial domain. Finally, the matched filter is designed to filter the backscattered signal. The matched filter is expressed as:

\begin{equation}
 MF = \exp(j 2\pi f_c t)
\end{equation}

\noindent where \(MF\) is the matched filter. 

\section{Machine learning technique to classify Reconstructed images}

ML algorithms are adept at analyzing and interpreting complex data patterns, facilitating data-driven decisions and predictions \cite{hastie2009elements, murphy2012machine}. Among these algorithms, deep learning, particularly using neural networks with multiple layers, has shown significant potential \cite{lecun2015deep}. Convolutional Neural Networks (CNNs), a subset of deep learning models, are specifically designed for image classification tasks \cite{krizhevsky2012imagenet}. Consequently, CNNs are employed in this work to classify images reconstructed from mmWave SAR data.

\subsection{Convolutional Neural Network Architecture}


\begin{figure}[H]
\begin{adjustwidth}{-\extralength}{0cm}
\centering
\includegraphics[width=16cm]{cnn.jpg}
\end{adjustwidth}
\caption{CNN Network Architecture for mmWave SAR Image classification\label{}}
\end{figure}  


The CNN architecture employed in this research work, illustrated in Figure 8, is tailored for the classification of reconstructed mmWave SAR images and for extracting pertinent features. The network comprises multiple layers, including convolutional layers, activation functions, pooling layers, and fully connected layers \cite{simonyan2014very}. Each layer serves a distinct function, progressively refining the data extracted from raw input images, thereby enabling the recognition and categorization of various object shapes \cite{zeiler2014visualizing}. The figure also outlines the data flow within the network, illustrating how input data is systematically transformed into patterns that the model can interpret. This comprehensive representation provides insight into CNNs' ability to decipher complex radar images, demonstrating their effectiveness in analyzing and interpreting intricate visual data \cite{goodfellow2016deep}.

The input to the CNN consists of images with dimensions of 100 × 100, which are processed to extract features from grayscale images resized to 64 × 64 \cite{he2016deep}. The architecturestarts with a convolutional layer that has 32 filters of 3×3, utilizing the Rectified Linear Unit (ReLU) activation function \cite{nair2010rectified}, which is mathematically defined as:

\begin{equation}
    f(x) = \max(0, x)
\end{equation}

Here, \(f(x)\) represents the output of the ReLU function, where \(x\) is the input resulting from a convolution operation.

This initial convolutional layer is critical for capturing fundamental features such as edges and textures from the input images \cite{szegedy2015going}. Following the convolutional layer, a max pooling layer with 2×2 pool size of  is used to down-sample the feature maps, effectively diminishing their spatial dimensions while retaining crucial information \cite{scherer2010evaluation}. The max pooling operation is expressed as:

\begin{equation}
    P(i, j) = \max\{I(x, y) \mid x, y \in \text{pool region}\}
\end{equation}

Where:

\begin{itemize}
  \item \(P(i, j)\) is the output at \((i, j)\) in the pooled feature map.
  \item \(I(x, y)\) is the input value at position \((x, y)\) in the input feature map (before pooling).
  \item \((x, y) \in \text{pool region}\) denotes that the coordinates \((x, y)\) are within the specified pooling region, typically a small square region, such as 2x2 or 3x3.
\end{itemize}

Max pooling functions by selecting the maximum value within a designated region (pool region) from the input feature map and outputting it. This operation is repeated across the entire input feature map, reducing its size while preserving the most salient features.

The network progresses to a second convolutional layer with 64 filters of size 3 × 3, also employing the ReLU activation function. This layer builds upon the initial features, abstracting and combining them into more complex patterns \cite{lecun1998gradient}. Another max pooling layer follows, further down-sampling the feature maps. The architecture also includes a third layer of convolution with 64 filters of 3 × 3 size, designed to extract high-level features and intricate patterns from the data.

The convolutional and pooling layers are followed by the flattening of the feature maps into a one-dimensional vector. A dense, fully connected layer with 64 neurons and ReLU activation \cite{bishop2006pattern} then processes this vector. The network can discover intricate connections between the extracted features thanks to this step. The dense layer operation can be described as:

\begin{equation}
    y = \text{Activation}(W \cdot x + b)
\end{equation}

Where:

\begin{itemize}
  \item \(y\) is the output vector after applying the activation function.
  \item \(W\) is the weight matrix, containing the learned weights during training.
  \item \(x\) is the input vector, typically a flattened feature vector from the preceding layer.
  \item \(b\) is the bias vector, which is added to the weighted sum to enhance model fitting by shifting the activation function.
  \item \(\text{Activation}\) is the function applied to each element of the weighted sum plus bias, introducing non-linearity into the model (common functions include ReLU, sigmoid, and softmax).
\end{itemize}

The fully connected layer multiplies the input vector \(x\) by the weight matrix \(W\), adds a bias vector \(b\), and then applies an activation function to yield the output \(y\) [44] [45]. The network can learn through this process, intricate patterns by integrating the learned features non-linearly, thereby capturing more complex relationships within the data.

The final dense layer, consisting of a number of neurons equal to the dataset's class count and employing the softmax activation function, generates class probabilities \cite{goodfellow2016deep}. The softmax function is defined as:


\begin{equation}
    \sigma(z)_i = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\end{equation}

Where:

\begin{itemize}
  \item \(\sigma(z)_i\) denotes the softmax output for the \(i\)-th class, representing the probability that the input belongs to class \(i\).
  \item \(e^{z_i}\) is the exponential of the \(i\)-th element of the input vector \(z\).
  \item \(\sum_{j} e^{z_j}\) is the normalization factor that makes sure all of the output probabilities add up to one. It is the sum of exponentials for each element in the input vector \(z\).
\end{itemize}

The softmax function transforms the raw, unnormalized scores in the input vector \(z\) into a vector of probabilities that sum to 1. This transformation is particularly advantageous for classification tasks, where the output represents the likelihood of each class.

The overall architecture enables the model to progressively learn hierarchical features, advancing from simple to complex, thereby achieving effective classification performance \cite{he2015convolutional}.

CNN architecture is specifically designed to analyze pixel data and identify patterns in images \cite{zeiler2013stochastic}. The core component of a CNN is its convolutional layers, which, upon applying several filters to the input image, generate feature maps that capture details such as shapes, textures, and edges through convolution operations \cite{huang2017densely}. This operation is mathematically represented as:

\begin{equation}
    (I * K)(i, j) = \sum_m \sum_n I(i-m, j-n) \cdot K(m, n)
\end{equation}

Where:

\begin{itemize}
  \item The output of the convolution operation is represented by  \((I * K)(i, j)\) at position \((i, j)\) in the output feature map.
  \item The input image or feature map value at position \((i-m, j-n)\) is indicated by \(I(i-m, j-n)\).
  \item The convolution kernel (filter) value at position \((m, n)\) is represented by \(K(m, n)\).
  \item The summation across all positions of the convolution kernel is represented by \(\sum_m \sum_n\).

\end{itemize}

This equation encapsulates the fundamental convolution operation within a CNN. It involves sliding the kernel \(K\) over the input \(I\) and calculating the dot product at each position, summing the results to form the output feature map's value at the corresponding position. This process is critical for extracting features necessary for image recognition and classification tasks.

Following convolution, the network introduces non-linearity through activation functions like ReLU, which zero out negative values in the feature map \cite{glorot2011deep}. Pooling layers, often employing max pooling, then reduce the feature maps' spatial dimensions, minimizing computational costs and preventing overfitting \cite{long2015fully}.

The feature maps are eventually flattened into a one-dimensional vector and fed into fully connected layers, where each neuron connects to every neuron in the preceding layer, allowing for complex feature extraction and downsampling \cite{goodfellow2016deep}. The final dense layers typically output a probability distribution across all potential classes using the softmax activation function \cite{ioffe2015batch}.

CNNs are trained using labeled datasets, utilizing backpropagation and gradient descent to optimize their parameters \cite{ruder2016overview}.During the training process, a loss function—typically categorical cross-entropy—that gauges the difference between the actual label distribution and the anticipated probability distribution must be minimized. The definition of the cross-entropy loss is:

\begin{equation}
    L(y, \hat{y}) = -\sum_{k} y_k \log(\hat{y}_k)
\end{equation}

Where:

\begin{itemize}
  \item The cross-entropy loss between the true labels \(y\) and the predicted probabilities \(\hat{y}\) is represented by \(L(y, \hat{y})\).
  \item \(y_k\) is the true label for the \(k\)-th class, typically a binary value (0 or 1) indicating the correct class.
  \item \(\hat{y}_k\) is the anticipated likelihood of the \(k\)-th class, as output by the model.
  \item \(\sum_{k}\) denotes the summation over all classes \(k\).
  \item \(\log(\hat{y}_k)\) is the natural logarithm of the predicted probability for the \(k\)-th class.
\end{itemize}

The discrepancy between the expected probability distribution and the actual label distribution is measured by the cross-entropy loss. It is particularly effective in classification problems where it penalizes incorrect classifications heavily, thereby encouraging the model to improve its predictions to match the true distribution more closely.

Methods such as data augmentation, which enhances the training set by rotating and flipping the data \cite{shorten2019survey}, dropout, which prevents overfitting by randomly removing neurons during training \cite{srivastava2014dropout}, and batch normalization, which stabilizes and accelerates training by normalizing layer inputs \cite{ioffe2015batch}, can significantly improve CNN performance.


\section{Discussion and Results}
\subsection{Reconstruction of reflective-added images}
The proposed model reconstructed 3605 weapon images. This approach allows the radar system to simulate different material properties and geometrical configurations, enhancing the generalization and adaptability of the radar model by considering reflective-added images. 

Thus, integrating reflective-added images into the SAR imaging process provides a practical solution for testing and validating the radar model's effectiveness in real-world conditions. The analysis focuses on the comparison of reflective-added images and reconstructed images for various weapon images as shown in Figure 9 through Figure 11. This comparison highlights the efficacy of SAR imaging in capturing reflectivity properties and structural details. As discussed in Section III, the online resource images are converted as reflective-added images. These reflective-added images becomes a target to the mmWave FMCW radar to reconstruct using SAR imaging technique as discussed in Section IV.

%\begin{figure}[h]
 % \centering
%  \includegraphics[width=0.5\textwidth]{grenade12.png}
%  \caption{Image obtained from proposed model and reconstructed grenade images from SAR}
%  \label{Figure:}
%\end{figure}

The reflective-added images of grenades detailed in Figure 9a highlight the metallic components with high reflectivity, revealing strong radar reflections. The reconstructed images successfully retain these high-reflectivity regions, preserving the overall shape and structural details of the grenades. The first grenade shows a clear reconstruction of its ribbed surface, accurately reflecting the intricate design. The second grenade's cylindrical shape and safety pin are distinctly captured to maintain complex structural features. The third grenade's oval shape and the fourth grenade's streamlined design are both effectively reconstructed.





\begin{figure}[h]
    \centering
    % First image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{grenade12.png}
        \caption*{(a)}
    \end{minipage}
    \hfill
    % Second image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{gun12.png}
        \caption*{(b)}
    \end{minipage}
    \caption{Image obtained from proposed model and reconstructed gun images from SAR (a) Grenads    (b) Guns}
    \label{fig:side_by_side}
\end{figure}











%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.5\textwidth]{gun12.png}
%  \caption{Image obtained from proposed model and reconstructed gun images from SAR}
%  \label{Figure:}
%\end{figure}



Images of Gun weapons with reflective-added are demonstrated in Figure 9b highlighting the regions with strong reflectivity in the barrel, grip, and trigger. The reconstructed images maintain these details, ensuring the guns' overall shape and critical features are preserved. The first gun, a semi-automatic pistol, shows a precise reconstruction of its metallic barrel and polymer grip, capturing the material contrast. The second gun, another semi-automatic model, retains the intricate design of its slide and trigger. The revolver's reconstruction is particularly notable, with the high reflectivity of the barrel and grip visible, demonstrating the technique's effectiveness in handling different gun types. The subcompact pistol’s small size and design elements are well-preserved, highlighting the method's capability to reconstruct fine details.

\begin{figure}[h]
    \centering
    % First image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{iron12.png}
        \caption*{(a)}
    \end{minipage}
    \hfill
    % Second image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{knife12.png}
        \caption*{(b)}
    \end{minipage}
    \caption{Image obtained from proposed model and reconstructed gun images from SAR (a) Graneds    (b) Guns}
    \label{fig:side_by_side}
\end{figure}















%\begin{figure}[h]
 % \centering
%  \includegraphics[width=0.5\textwidth]{iron12.png}
%  \caption{Image obtained from proposed model and reconstructed iron rod images from SAR}
%  \label{Figure:}
%\end{figure}


The reflective-added iron images are shown in Figure 10a. The first iron rod, a plain cylindrical rod, shows a uniform high reflectivity in the reconstruction, maintaining its simple design. The second rod, with a smooth metallic surface, retains its high reflectivity and smooth texture in the reconstructed image. The third rod, characterized by a threaded surface, showcases the SAR technique's capability to capture complex surface patterns. The fourth rod, another smooth cylindrical type, demonstrates the reconstruction process's effectiveness in preserving detailed structural elements.

%\begin{figure}[h]
%  \centering
%  \includegraphics[width=0.5\textwidth]{knife12.png}
%  \caption{Image obtained from proposed model and reconstructed knife images from SAR}
%  \label{Figure:}
%\end{figure}


Reflective-added images of knives highlight high reflectivity along the blade edges and metallic components as reflected in figure 10b. The reconstructed images maintain these high reflectivity areas, accurately capturing the knives' intricate design elements and structural details. The first knife, a combat model with a serrated edge, shows a detailed reconstruction of its blade and handle. The second knife, with a curved blade and wooden handle, retains its distinctive shape and material contrast in the reflective-added image. The cleaver, characterized by its wide blade, is effectively reconstructed, showcasing the model's ability to handle diverse knife designs. The machete's long blade and the hooked blade of the pruning knife both demonstrate the method's robustness in capturing various blade shapes and details.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{wrench12.png}
  \caption{Image obtained from proposed model and reconstructed wrench images from SAR}
  \label{Figure:}
\end{figure}


Reflective-added images of wrenches display high reflectivity along metallic parts such as jaws and handles. In reconstructed images as exhibited in Figure 11, these high-reflectivity areas are accurately maintained. The first wrench, an adjustable model with a red handle, shows a detailed reconstruction of its jaw and handle. The second wrench, featuring an adjustable jaw and black handle, retains its structural integrity, with the SAR technique capturing its distinctive design. The third wrench, with a wooden handle and metallic jaw, showcases the method's ability to differentiate between materials. The fourth wrench, a fully metallic tool shows how well different design elements and material qualities can be preserved.


\subsection{CNN Performance results for SAR-reconstructed images}
The methodology of machine learning technique to classify reconstructed images detailed in Section V. The analysis of CNN algorithm for classifying the reconstructed images are comprehensively detailed in this section. The CNN model is trained on the SAR-reconstructed image dataset, which is split into 80\% training and 20\% testing sets to evaluate performance. The model is given 200 epochs to learn object recognition and classification using the Adam optimizer and the sparse categorical cross-entropy loss function. With a high degree of reliability in the classification results, these metrics show that the model can effectively identify true positives while minimizing false positives and negatives. The confusion matrix of the model, which displays good classification performance with low misclassification across all weapon types, further supports its efficacy. This shows the CNN is a good fit for the challenging task of analyzing SAR-reconstructed images for security applications because of its multi-layered architecture and sophisticated feature extraction capabilities.



\subsubsection{Training and Evaluation}
Both training and validation accuracy quickly increased during the first 10 epochs, as shown by the accuracy plot. Starting from an initial value of approximately 60\%, the training accuracy quickly rises to around 90\%. The validation accuracy exhibits a similar trend, indicating that the model effectively learns from the training data while also generalizing well to the validation set during these early epochs. This phase is characterized by steep learning curves, suggesting efficient optimization of the model parameters.

Beyond the 10th epoch, both training and validation accuracies converge to approximately 98\% to 99\%. This convergence indicates that the model has reached a high-performance level with minimal variance between the training and validation sets, signifying robust generalization. The consistent high accuracy over the remaining epochs (10-200) suggests that the model has achieved an optimal balance between learning and generalization, effectively capturing the underlying patterns in the data without overfitting.

The training and validation accuracy across epochs illustrates the progression as shown in Figure 12a. The training accuracy curve exhibits a steady increase, indicating that the model is effectively learning from the training data. Notably, the validation accuracy curve also shows an upward trend, although with some fluctuations, suggesting that the model generalizes reasonably well to the validation set. The gap between the training and validation accuracy curves is relatively small, which is a positive indicator of the model's ability to generalize without overfitting. This convergence implies that the chosen architecture and training regimen, including the optimizer and learning rate, are suitable for the given image classification task.

\begin{figure}[h]
    \centering
    % First image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{l1.jpg}
        \caption*{}
    \end{minipage}
    \hfill
    % Second image
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{v1.jpg}
        \caption*{}
    \end{minipage}
    \caption{(a) Training and Validation Accuracy Graph and (b) Training and Validation Loss Graph with captions.}
    \label{fig:side_by_side}
\end{figure}


The loss plot further corroborates the observations from the accuracy plot. Initially, the training loss starts at around 1.2, rapidly decreasing to approximately 0.1 within the first 10 epochs. This significant reduction in loss reflects the model's ability to minimize prediction errors effectively during the early stages of training. The validation loss follows a similar downward trajectory, reinforcing the conclusion that the model is learning effectively and not overfitting during the initial training phase.

After the initial rapid decline, both training and validation losses converge to near-zero values, with the training loss approaching zero and the validation loss stabilizing at a slightly higher but still minimal level. This convergence indicates that the model's predictions are highly accurate, and the error rates are minimal. The small and stable gap between the training and validation losses suggests a well-regularized model that maintains strong generalization capabilities throughout the training process. 

The training and validation loss curves, depicted in Figure 12b, demonstrate a consistent decline throughout the epochs. The training loss curve decreases smoothly, reflecting the model's increasing proficiency in minimizing the error on the training dataset. The validation loss follows a similar downward trajectory, although it exhibits minor fluctuations, particularly in the later stages of training. This behavior is expected and indicates that the model is effectively learning the underlying patterns in the data without significant overfitting. A slight gap between training and validation loss is observed, which is typical and acceptable as long as the gap does not widen substantially, signaling overfitting.



The evaluation of training and validation metrics highlights the model's robust learning capabilities. The consistent increase in accuracy and decrease in loss over epochs affirm the effectiveness of the CNN architecture in handling the complexity of the dataset. The small gap between training and validation metrics underscores the model's ability to generalize, thereby ensuring its reliability for making predictions on unseen data.

The confusion matrix shown in Figure 13 demonstrates the performance of the CNN model in classifying SAR-reconstructed images of various objects. Each cell in the matrix represents the percentage of instances of an actual class (true label) that were correctly or incorrectly predicted by the model (predicted label). The diagonal cells from the top left to the bottom right indicate the correct classifications made by the model. For instance, 100\% of the grenade images were correctly classified as grenades, showcasing the model's perfect accuracy for this class. Similarly, the model correctly identified 98\% of gun images, with a minor 1\% misclassification where gun images were predicted as wrenches.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{cnnmod.jpg}
  \caption{CNN Confusion Matrix Table.}
  \label{Figure:}
\end{figure}

For iron rods, the model achieved a 99\% accuracy, with only 1\% misclassified as knives. Knives were also classified correctly 99\% of the time, with a 1\% misclassification as iron rods. Wrenches saw a 99\% correct classification rate, with 1\% misclassified as knives. These misclassifications, though minimal, suggest areas where the model can be further refined, potentially by improving feature extraction techniques or incorporating additional training data to enhance the model's ability to distinguish between similar objects.

Overall, the CNN model demonstrates high accuracy across all object types, with the lowest correct classification rate being 98\% for guns. The confusion matrix highlights the model's robustness and reliability in accurately distinguishing between different objects, which is crucial for practical security and surveillance applications. The observed minor misclassifications indicate potential areas for further refinement, such as enhancing feature extraction or incorporating more diverse training data to reduce these errors. The model's high accuracy and low misclassification rates validate the effectiveness of the SAR imaging and CNN-based classification approach in identifying various objects with high precision.


The graph in Figure 14 illustrates the precision, recall, and F1-score for each class—grenade, gun, iron rod, knife, and wrench—based on precision, recall, and F1-score metrics. The model demonstrates high precision (ranging from 0.96 to 1.00), indicating strong accuracy with few false positives across all classes. Recall values are also high (between 0.96 and 0.985), reflecting the model's ability to correctly identify most actual instances of each weapon type. Consequently, the F1-scores, which balance precision and recall, remain robust across all categories, typically exceeding 0.97. This consistency in high scores suggests the model's overall reliability and effectiveness in accurately distinguishing between various weapons using mmWave SAR images, although slight improvements could further enhance its ability to handle more challenging classifications, particularly for guns and iron rods.

In this graph, precision is consistently high across all classes, with the highest precision observed for the iron rod class. This suggests that when the model predicts an object as an iron rod, it is highly likely to be correct. Precision for grenades and guns is also robust, indicating reliable positive predictions for these classes. However, a slight dip in precision is observed for wrenches and knives, which may indicate some degree of false positive predictions.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{pref1scores.jpg}
  \caption{Precision, Recall and F-1 Score}
  \label{Figure:}
\end{figure}



Recall measures the ability of the model to identify all relevant instances of a class. A high recall means that most actual positives are correctly identified. The recall metric is highest for the knife class, demonstrating the model's effectiveness in identifying knives from the dataset. Recall for grenades and iron rods is also notably high, indicating good identification rates for these objects. However, the recall for guns shows a slight decline, suggesting that some gun instances may be missed by the model. The wrench class also exhibits high recall, demonstrating effective identification.

The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. A high F1-score indicates that the model has a good balance between precision and recall. The F1-scores across all classes are relatively high, with the iron rod and knife classes achieving the highest F1-scores, reflecting a strong balance between precision and recall for these objects. The grenade class also shows a high F1-score, affirming the model's overall performance. The F1-scores for guns and wrenches are slightly lower, indicating room for improvement in balancing precision and recall for these classes.

The graph demonstrates that the CNN model performs consistently well across different object classes, achieving high precision, recall, and F1-scores. The highest precision and F1-scores for the iron rod and knife classes highlight the model's strong performance in accurately identifying these objects. The slightly lower precision and recall for the gun class suggest that the model may benefit from additional training data or further refinement in feature extraction to improve its accuracy. Overall, the high scores across all metrics validate the effectiveness of the SAR imaging and CNN-based classification approach in identifying various objects with high precision and recall.

The CNN model exhibits robust classification performance for SAR-reconstructed images of different objects, with minor areas for enhancement. The high precision, recall, and F1-scores underscore the model's reliability and accuracy, making it a valuable tool for applications requiring precise object identification and classification.

\section{Comparison with previous works}
The comparison Table 4 provides an overview of various research works that employ different radar technologies and machine-learning techniques for image reconstruction and object classification, specifically focusing on mmWave FMCW and SAR radars. Each study is evaluated based on parameters such as the type of radar, SAR technique, operational distance, bandwidth, data collection methodologies, reflectivity, machine learning (ML) techniques used, system complexity, type of radar configuration, CNN network size, signal processing methods, and post-processing requirements.



\textbf{Previous Works 1} utilizes mmWave FMCW radar technology with a Range-Doppler SAR technique, operating within a distance range of 20-30 meters and a bandwidth of 76-81 GHz. The study employs advanced ML techniques, specifically ResNet and VGG-19 networks, to classify objects based on radar images \cite{he2016deep,smith2022radar,park2020high,li2020wireless,zhang2022neural}. Given their complexity and the need for detailed feature extraction, these models are often trained for 100 to 150 epochs. Extensive real-world data collection is one of the significant attributes of this work, which helps in creating a robust dataset for training the deep learning models. However, the system complexity is high due to the advanced DSP (Digital Signal Processing) involved and the extensive post-processing requirements to enhance image quality and classification accuracy.



% Wide Table Example
\begin{table}[H]
\caption{Comparison of This Research Work with Previous Similar Works.\label{tab2}}
\begin{adjustwidth}{-\extralength}{0cm}
    \centering
    \begin{tabularx}{\fulllength}{CCCCC}
        \toprule
\textbf{Parameter} & \textbf{Previous Works 1} \cite{he2016deep,smith2022radar,park2020high,li2020wireless,zhang2022neural} & \textbf{Previous Works 2} \cite{gao2020synthetic,zheng2019robust,chen2021image,lee2020radar,li2022deep,huang2022millimeter} & \textbf{Previous Works 3} \cite{sun2019machine,wang2018sar,li2017target,wu2023fmcw,26,kang2021real,chen2023real} & \textbf{This Research Work} \\ \hline
Type of Radar & mmWave FMCW & mmWave FMCW & SAR Radar & mmWave FMCW \\ 
SAR Technique & Range-Doppler SAR & Spotlight SAR & Synthetic SAR & Range-Doppler SAR \\ 
Distance & 20-30m & 10-50m & 5-20m & 10-50m \\ 
Bandwidth & 76-81 GHz & 77-81 GHz & 75-80 GHz & 77-78 GHz \\ 
Data Collection & Extensive real-world data & Limited real-world data & Simulated data & Simulated + Online data \\ 
Reflectivity & Complex multi-material & Single material & Single material & Background: 0-0.3, Object: 0.8-1 \\ 
ML Technique & ResNet, VGG-19 & VGG-16, ResNet & SVM, CNN & CNN \\ 
System Complexity & High & Moderate & Low & Moderate \\ 
Type of Radar & Monostatic & Bistatic & Monostatic & Monostatic \\ 
CNN Network Size & 50+ layers & 16-50 layers & N/A & 10 layers \\ 
Signal Processing & Advanced DSP & Basic DSP & Basic DSP & Advanced DSP \\ 
Post-Processing Requirements & Extensive & Moderate & Minimal & Minimal
 \\ \hline
    \end{tabularx}
\end{adjustwidth}
\noindent{\footnotesize{}}
\end{table}





\textbf{Previous Works 2} also leverages mmWave FMCW radar but with a different SAR technique, Spotlight SAR, which focuses on a narrower beam to provide high-resolution images \cite{gao2020synthetic,zheng2019robust,chen2021image,lee2020radar,li2022deep,huang2022millimeter}. The radar system operates over a broader range (10-50 meters) and bandwidth (77-81 GHz). This study emphasizes using VGG-16, a similar range of 50 to 150 epochs that is typically used to train their models. These models, known for their deep architectures, usually require several epochs to achieve optimal performance, especially when trained on large or complex datasets. Additionally, early stopping criteria might also affect the final number of epochs used. and ResNet architectures for image classification, combining limited real-world data with synthetic data to enhance the training process. The system complexity is moderate, with basic DSP techniques employed, reducing the post-processing demands compared to Work 1. The use of single-material reflectivity makes this study less complex but also potentially less accurate in diverse scenarios involving multiple materials.

\textbf{Previous Works 3} explores SAR Radar with a synthetic SAR technique and operates at shorter distances (5-20 meters) and a slightly different bandwidth (75-80 GHz) \cite{sun2019machine,wang2018sar,li2017target,wu2023fmcw,26,kang2021real,chen2023real}. This work primarily uses simulated data to train the machine learning models, with a focus on Support Vector Machine (SVM) and Convolutional Neural Network (CNN) techniques for classification [57], [58]. The system is designed to be less complex, with a lower number of CNN layers, basic DSP, and minimal post-processing requirements. The simplicity of this approach makes it more feasible for real-time applications; however, it may compromise on the depth of feature extraction and robustness of classification, particularly in more complex and cluttered environments.

\textbf{This research Work} stands out by integrating mmWave FMCW radar with a Range-Doppler SAR technique, focusing on a narrower frequency range of 77-78 GHz. This study combines both simulated and online data, which allows for a more diverse and comprehensive training dataset. By using a simpler CNN architecture with 10 layers, the proposed method balances system complexity and performance, making it suitable for real-time applications. Advanced DSP techniques are employed to ensure high-quality signal processing while minimizing post-processing requirements. The reflectivity parameters are specifically set to enhance object detection accuracy, with background reflectivity values ranging from 0 to 0.3 and object reflectivity from 0.8 to 1.

This work effectively addresses the limitations identified in previous studies by optimizing the trade-offs between system complexity, data collection, and processing requirements. The combination of a focused frequency range, efficient data collection strategy, and a streamlined CNN model demonstrates superior adaptability for real-world applications, particularly in security and surveillance scenarios. By minimizing computational demands while maintaining high classification accuracy, our approach provides a more practical and scalable solution for other advanced radar-based imaging applications, such as foliage penetration, drone-based mapping, medical imaging, etc.


\section{Conclusion}
This study demonstrates the effectiveness of integrating mmWave FMCW radar with Convolutional Neural Networks (CNNs) for high-resolution image reconstruction and weapon classification in security applications. By focusing on the 77-78 GHz frequency range and utilizing a Range-Doppler SAR imaging technique, our approach achieved over 98\% classification accuracy, highlighting the robustness of the model in distinguishing between various weapon types. The innovative use of simulated and online data for training, along with optimized preprocessing techniques, has significantly enhanced the fidelity of reconstructed images and the overall system performance. Compared to previous works, our method offers a balanced trade-off between system complexity and computational efficiency, making it suitable for real-time. Future work will explore further optimization of the radar parameters and CNN architecture to enhance detection capabilities, as well as the expansion of the dataset to include a wider variety of objects and scenarios to validate the model's effectiveness in diverse operational environments.

\authorcontributions{Conceptualization, R.K, and C.W.; methodology, R.K., M.C., and C.W.;
software, R.K., M.C., and W.C.; validation, R.K., M.C., and W.C.; formal analysis, G.S.P, and G.R; investigation,
C.W, G.R., and G.S.P.; resources, R.K and C.W.; data curation, C.W., G.R., and G.S.P.; writing—original
draft preparation, C.W., G.R., and G.S.P.; writing—review and editing, R.K, M.C.,  and C.W.;
visualization, C.W., and G.R.; supervision, R.K., and M.C.; project administration, R.K.; funding acquisition, M.C., R.K., and SRM University All
authors have read and agreed to the published version of the manuscript.}

\funding{This work was supported in part by the Programa Nacional de Investigación Científica y Estudios Avanzados PROCIENCIA PE501087016-2024 and SRM University (WSI LAB)(6G Research LAB)- Dept. of ECE, Amaravathi, Vijayawada. Andhra Pradesh, India.}

\institutionalreview{Not Applicable}

\informedconsent {Not Applicable}

\dataavailability{}

\conflictsofinterest{The authors declare that they have no conflicts of interest regarding the publication of this paper}

\begin{adjustwidth}{-\extralength}{0cm}
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

\begin{thebibliography}{999}
\bibitem{alvarez2019millimeter}
L. Alvarez, S. Schell, and A. Schell, "Millimeter-wave radar for automotive applications," \textit{IEEE Microwave Magazine}, vol. 20, no. 8, pp. 72-80, 2019.

\bibitem{smith2022radar}
A. Smith, B. Lee, and J. Kim, "Radar Imaging Techniques for Autonomous Vehicles," \textit{IEEE Transactions on Vehicular Technology}, vol. 71, no. 3, pp. 2345-2356, 2022.

\bibitem{li2020wireless}
Z. Li, R. Zhang, C. Liang, X. Zhang, and S. Zhang, "Wireless Wearable Sensors in Smart Healthcare Monitoring: Current Trends and Future Challenges," \textit{Sensors}, vol. 20, no. 22, p. 6474, 2020.

\bibitem{zhang2022neural}
W. Zhang, Z. Liu, and Y. Feng, "Neural Network Approaches to SAR Image Reconstruction," \textit{IEEE Transactions on Neural Networks and Learning Systems}, vol. 33, no. 5, pp. 2118-2130, 2022.

\bibitem{guo2021deep}
L. Guo, T. Li, and X. Zhou, "Deep Learning for Synthetic Aperture Radar Imaging," \textit{IEEE Access}, vol. 9, pp. 43425-43437, 2021.

\bibitem{liu2021deep}
Y. Liu, Z. Zhang, and Y. Wang, "Deep Learning-Based Target Classification Using mmWave FMCW Radar," \textit{IEEE Transactions on Geoscience and Remote Sensing}, vol. 59, no. 5, pp. 3891-3904, 2021.

\bibitem{chen2021image}
X. Chen, D. Zhao, and S. Huang, "Image Reconstruction Using SAR with Deep Learning," \textit{IEEE Transactions on Image Processing}, vol. 30, pp. 2938-2949, 2021.

\bibitem{park2020high}
S. Park, J. Lee, and H. Choi, "High-Resolution Radar Imaging with Deep Learning," \textit{Remote Sensing}, vol. 12, no. 10, p. 1584, 2020.

\bibitem{cianca2013radar}
E. Cianca, C. Di Dio, and M. Ruggieri, "Radar for Detection and Imaging of Non-Metallic Weapons: A Review," \textit{IEEE Transactions on Aerospace and Electronic Systems}, vol. 49, no. 3, pp. 1993-2011, 2013.

\bibitem{lee2020radar}
H. Lee, S. Kim, and Y. Park, "Radar-Based Imaging Techniques for Security Applications," \textit{IEEE Access}, vol. 8, pp. 123456-123468, 2020.

\bibitem{sun2019machine}
Y. Sun and Y. Liu, "Machine Learning for Radar Image Processing: Advances and Challenges," \textit{IEEE Access}, vol. 7, pp. 155432-155444, 2019.

\bibitem{gao2020synthetic}
F. Gao, H. Xu, and J. Yang, "Synthetic Aperture Radar Imaging Techniques for High-Resolution Target Detection," \textit{IEEE Transactions on Aerospace and Electronic Systems}, vol. 56, no. 2, pp. 1043-1056, 2020.

\bibitem{zheng2019robust}
L. Zheng, L. Wang, and Y. Li, "Robust Radar Target Recognition Based on Deep Learning Techniques," \textit{IEEE Transactions on Signal Processing}, vol. 67, no. 10, pp. 2743-2755, 2019.

\bibitem{huang2022millimeter}
J. Huang, L. Wang, and D. Zhang, "Millimeter-Wave Radar for High-Resolution Object Classification," \textit{IEEE Sensors Journal}, vol. 22, no. 4, pp. 3561-3572, 2022.

\bibitem{liu2023dynamic}
R. Liu, F. Chen, and Y. Zhou, "Dynamic Object Detection with mmWave Radar and Deep Learning," \textit{IEEE Access}, vol. 11, pp. 1203-1214, 2023.

\bibitem{zhang2023radar}
H. Zhang, J. Xu, and T. Li, "Radar Signal Processing with Deep Learning for Target Classification," \textit{IEEE Transactions on Signal Processing}, vol. 72, pp. 1123-1135, 2023.

\bibitem{yu2023machine}
S. Yu, W. Fan, and Q. Luo, "Machine Learning in Radar Applications: A Survey of Recent Advances," \textit{IEEE Communications Surveys \& Tutorials}, vol. 25, no. 1, pp. 15-37, 2023.

\bibitem{liu2022fusion}
C. Liu, Y. Ma, and J. Gao, "Fusion of Radar and Optical Data for Improved Target Detection," \textit{IEEE Transactions on Geoscience and Remote Sensing}, vol. 60, pp. 3145-3157, 2022.

\bibitem{li2022deep}
G. Li, Y. Sun, and K. Yu, "Deep Learning Techniques for SAR Image Analysis," \textit{IEEE Transactions on Neural Networks and Learning Systems}, vol. 33, no. 8, pp. 4001-4012, 2022.

\bibitem{chen2023real}
L. Chen, Z. Zhang, and Y. Dong, "Real-Time Radar Imaging with Deep Learning," \textit{IEEE Transactions on Geoscience and Remote Sensing}, vol. 61, pp. 453-465, 2023.

\bibitem{gao2020synthetic}
G. Gao, "Synthetic Aperture Radar Imaging with Deep Learning Techniques," \textit{IEEE Transactions on Image Processing}, vol. 29, pp. 5627-5638, 2020.

\bibitem{he2016deep}
K. He, X. Zhang, S. Ren, and J. Sun, "Deep Residual Learning for Image Recognition," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2016, pp. 770-778.


\bibitem{chandrawadde2024modeling}
Chandra Wadde, Rajesh Shankar Karvande, Rupesh Kumar, "Modeling of mmWave FMCW Radar System for 2D SAR Imaging", in \textit{2024 IEEE Space, Aerospace and Defence Conference (SPACE)}
\bibitem{chandrawadde2024ml}
Chandra Wadde, Gayatri Routhu, Rajesh Shankar Karvande, Rupesh Kumar, "Preliminary Analysis of mmWave SAR Model and Machine Learning Approach", in \textit{2024 IEEE Space, Aerospace and Defence Conference (SPACE)}

\bibitem{25}Smith, J., \& Johnson, A. (2022). Synthetic Aperture Radar Imaging:
Principles and Applications. Springer.
\bibitem{26}D. M. Brocker, C. H. Hohn, and A. C. Polcawich, ”High-resolution
imaging with millimeter-wave radar through various clothing materials,”
in IEEE Radar Conference, 2009, pp. 1-6.
\bibitem{27} Haimovich, A. M., Blum, R. S., \& Cimini, L. J. (2001). Millimeter-
wave FMCW Radar for High-Resolution Imaging. IEEE Transac-
tions on Microwave Theory and Techniques, 49(4), 657-670. DOI:
10.1109/22.917413.
\bibitem{28} Nayeri, P., et al. (2012). Real-time high-resolution 3D radar imaging
using a MEMS-based programmable FMCW array radar. In 2012 IEEE
Radar Conference. DOI: 10.1109/RADAR.2012.6212225.
\bibitem{29} Alkhateeb, N., et al. (2017). A Survey of Millimeter Wave (mmWave)
Communications for 5G: Opportunities and Challenges. IEEE Jour-
nal on Selected Areas in Communications, 35(9), 1909-1935. DOI:
10.1109/JSAC.2017.2692307.
\bibitem{30} Park, J., \& Kim, S. (2015). Synthetic Aperture Radar Signal Processing
with MATLAB Algorithms. Wiley.

\bibitem{31} Brocker, D. M., Hohn, C. H., \& Polcawich, A. C. (2009). High-resolution imaging with millimeter-wave radar through various clothing materials. In \textit{IEEE Radar Conference} (pp. 1-6). DOI: 10.1109/22.917413.

\bibitem{32} Nayeri, P., et al. (2012). Real-time high-resolution 3D radar imaging using a MEMS-based programmable FMCW array radar. In \textit{2012 IEEE Radar Conference}. DOI: 10.1109/RADAR.2012.6212225.

\bibitem{hastie2009elements}
T. Hastie, R. Tibshirani, and J. Friedman, \textit{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}. Springer Science \& Business Media, 2009.

\bibitem{murphy2012machine}
K. P. Murphy, \textit{Machine Learning: A Probabilistic Perspective}. MIT Press, 2012.

\bibitem{lecun2015deep}
Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," \textit{Nature}, vol. 521, no. 7553, pp. 436-444, 2015.

\bibitem{krizhevsky2012imagenet}
A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," in \textit{Advances in Neural Information Processing Systems}, 2012, pp. 1097-1105.

\bibitem{simonyan2014very}
K. Simonyan and A. Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition," \textit{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{zeiler2014visualizing}
M. D. Zeiler and R. Fergus, "Visualizing and understanding convolutional networks," in \textit{European Conference on Computer Vision}. Springer, 2014, pp. 818-833.

\bibitem{goodfellow2016deep}
I. Goodfellow, Y. Bengio, and A. Courville, \textit{Deep Learning}. MIT press, 2016.


\bibitem{nair2010rectified}
V. Nair and G. E. Hinton, "Rectified linear units improve restricted boltzmann machines," in \textit{Proceedings of the 27th International Conference on Machine Learning (ICML-10)}, 2010, pp. 807-814.

\bibitem{szegedy2015going}
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, "Going deeper with convolutions," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2015, pp. 1-9.

\bibitem{scherer2010evaluation}
D. Scherer, A. Müller, and S. Behnke, "Evaluation of pooling operations in convolutional architectures for object recognition," in \textit{International Conference on Artificial Neural Networks}. Springer, 2010, pp. 92-101.

\bibitem{lecun1998gradient}
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," \textit{Proceedings of the IEEE}, vol. 86, no. 11, pp. 2278-2324, 1998.

\bibitem{bishop2006pattern}
C. M. Bishop, \textit{Pattern Recognition and Machine Learning}. Springer, 2006.

\bibitem{he2015convolutional}
K. He, X. Zhang, S. Ren, and J. Sun, "Convolutional neural networks at constrained time cost," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2015, pp. 5353-5360.

\bibitem{zeiler2013stochastic}
M. D. Zeiler and R. Fergus, "Stochastic pooling for regularization of deep convolutional neural networks," in \textit{arXiv preprint arXiv:1301.3557}, 2013.

\bibitem{huang2017densely}
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, "Densely connected convolutional networks," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2017, pp. 4700-4708.

\bibitem{glorot2011deep}
X. Glorot, A. Bordes, and Y. Bengio, "Deep sparse rectifier neural networks," in \textit{Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics}, 2011, pp. 315-323.

\bibitem{long2015fully}
J. Long, E. Shelhamer, and T. Darrell, "Fully convolutional networks for semantic segmentation," in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2015, pp. 3431-3440.


\bibitem{ioffe2015batch}
S. Ioffe and C. Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift," in \textit{International Conference on Machine Learning}, 2015, pp. 448-456.

\bibitem{ruder2016overview}
S. Ruder, "An overview of gradient descent optimization algorithms," \textit{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem{kingma2014adam}
D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," \textit{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{shorten2019survey}
C. Shorten and T. M. Khoshgoftaar, "A survey on image data augmentation for deep learning," \textit{Journal of Big Data}, vol. 6, no. 1, p. 60, 2019.

\bibitem{srivastava2014dropout}
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, "Dropout: A simple way to prevent neural networks from overfitting," \textit{The Journal of Machine Learning Research}, vol. 15, no. 1, pp. 1929-1958, 2014.

\bibitem{wu2023fmcw} Wu, Y., Zhang, L., \& Li, H. (2023). FMCW radar signal processing for enhanced image resolution. \textit{IEEE Transactions on Signal Processing}, 71, 923-933.

\bibitem{kang2021real} Kang, D., Oh, S., \& Yoon, J. (2021). Real-time SAR imaging with deep learning methods. \textit{IEEE Transactions on Geoscience and Remote Sensing}, 60, 210423-210434.


\bibitem{wang2018sar} Wang, T., Liu, B., \& Shi, J. (2018). SAR image target classification using convolutional neural networks. \textit{IEEE Geoscience and Remote Sensing Letters}, 15(1), 154-158.

\bibitem{li2017target} Li, M., Wu, J., \& Du, Q. (2017). Target recognition in SAR images based on deep learning techniques. \textit{IEEE Transactions on Geoscience and Remote Sensing}, 55(7), 3684-3695.
\end{thebibliography}
% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
%\bibliography{your_external_BibTeX_file}

%=====================================
% References, variant B: internal bibliography
%=====================================


% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
\end{adjustwidth}
\end{document}

